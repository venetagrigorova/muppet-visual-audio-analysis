{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57f1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, glob, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "from skimage import color\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# config\n",
    "SEED = 12332287\n",
    "np.random.seed(SEED)\n",
    "\n",
    "CLASSES = [\"MissPiggy\", \"OtherPigs\", \"SwedishChef\", \"Rowlf\"]\n",
    "\n",
    "# helpers\n",
    "def list_files(directory):\n",
    "    \"\"\"list files in a folder\"\"\"\n",
    "    return sorted(glob.glob(directory + \"/*\"))\n",
    "\n",
    "GT_REQUIRED = [\n",
    "    \"Video\", \"Frame_number\", \"Timestamp\",\n",
    "    \"Kermit\", \"Pigs\", \"Miss Piggy\", \"Cook\",\n",
    "    \"StatlerWaldorf\", \"Rowlf the Dog\", \"Fozzie Bear\"\n",
    "]\n",
    "\n",
    "GT_TO_INTERNAL = {\n",
    "    \"Miss Piggy\": \"MissPiggy\",\n",
    "    \"Pigs\": \"OtherPigs\",\n",
    "    \"Cook\": \"SwedishChef\",\n",
    "    \"Rowlf the Dog\": \"Rowlf\",\n",
    "}\n",
    "\n",
    "def read_ground_truth(gt_dir):\n",
    "    \"\"\"reads all ground truth xlsx files and returns df indexed by (Video, Frame_number)\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(gt_dir, \"*.xlsx\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"no ground truth xlsx found in gt_dir\")\n",
    "\n",
    "    dfs = []\n",
    "    for path in files:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "        # verify headers\n",
    "        missing = [c for c in GT_REQUIRED if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{os.path.basename(path)} missing columns: {missing}\")\n",
    "\n",
    "        # enforce numeric dtypes\n",
    "        df[\"Video\"] = pd.to_numeric(df[\"Video\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"Frame_number\"] = pd.to_numeric(df[\"Frame_number\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # map exact GT columns to internal class names\n",
    "        for gt_col, internal in GT_TO_INTERNAL.items():\n",
    "            df[internal] = pd.to_numeric(df[gt_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        dfs.append(df[[\"Video\", \"Frame_number\"] + list(GT_TO_INTERNAL.values())])\n",
    "\n",
    "    gt = pd.concat(dfs, ignore_index=True)\n",
    "    gt = gt.dropna(subset=[\"Video\", \"Frame_number\"])\n",
    "    gt[\"Video\"] = gt[\"Video\"].astype(int)\n",
    "    gt[\"Frame_number\"] = gt[\"Frame_number\"].astype(int)\n",
    "\n",
    "    gt = gt.set_index([\"Video\", \"Frame_number\"]).sort_index()\n",
    "\n",
    "    for c in CLASSES:\n",
    "        if c not in gt.columns:\n",
    "            gt[c] = 0\n",
    "        gt[c] = gt[c].astype(int)\n",
    "\n",
    "    return gt\n",
    "\n",
    "def get_label(video_id, frame_idx, gt_df):\n",
    "    \"\"\"\n",
    "    return one of the internal labels in CLASSES or None\n",
    "    based on the ground truth table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = gt_df.loc[(video_id, frame_idx)]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "    # row contains columns: MissPiggy, OtherPigs, SwedishChef, Rowlf\n",
    "    # only accept frames where exactly one of these is 1\n",
    "    vals = row[CLASSES].values.astype(int)\n",
    "    if vals.sum() != 1:\n",
    "        return None\n",
    "\n",
    "    label_idx = vals.argmax()\n",
    "    return CLASSES[label_idx]\n",
    "\n",
    "def video_iter_frames(path):\n",
    "    \"\"\"go through video frame by frame and yield (idx, frame_bgr, fps, nframes)\"\"\"\n",
    "    capture = cv2.VideoCapture(path) # open video\n",
    "    if not capture.isOpened():\n",
    "        raise RuntimeError(f\"cannot open video: {path}\")\n",
    "\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS) # frames per second\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_index = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = capture.read() # read next frame\n",
    "            if not ok:\n",
    "                break\n",
    "            yield frame_index, frame, fps, total_frames\n",
    "            frame_index += 1\n",
    "    finally:\n",
    "        capture.release()\n",
    "\n",
    "def video_to_audio(video_path):\n",
    "    \"\"\"extract audio from video and save as wav file\"\"\"\n",
    "    fname = os.path.basename(video_path).replace(\".avi\", \".wav\")\n",
    "    audio_out = os.path.join(\"data/audio\", fname)  # save in data/audio\n",
    "\n",
    "    # load video file and extract audio\n",
    "    video = mp.VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_out, codec=\"pcm_s16le\")\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d2464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = read_ground_truth(\"./data/ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def21c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in data/audio/211.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/244.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/343.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# separating audio from video\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "for v in video_paths:\n",
    "    video_to_audio(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e84a0",
   "metadata": {},
   "source": [
    "### Visual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4abd2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_frame(gray_frame):\n",
    "    \"\"\"extract HOG features from a single grayscale frame\"\"\"\n",
    "    h = hog(gray_frame, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))\n",
    "    return h\n",
    "\n",
    "\n",
    "def extract_lbp_frame(gray_frame, P=8, R=1):\n",
    "    \"\"\"extract LBP features from a single grayscale frame\"\"\"\n",
    "    lbp = local_binary_pattern(gray_frame, P, R, method=\"uniform\") # compute lbp codes for each pixel\n",
    "\n",
    "    # lbp produces values in [0, P+1], build histogram as the feature\n",
    "    bins = P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def extract_glcm_frame(gray_frame):\n",
    "    \"\"\"extract GLCM features from a single grayscale frame\"\"\"\n",
    "    # compute grey-level co-occurrence matrix for distance=1, angle=0\n",
    "    gc = graycomatrix(gray_frame, [1], [0], symmetric=True, normed=True) \n",
    "\n",
    "    # extract two common texture measures: contrast + homogeneity\n",
    "    return np.array([\n",
    "        graycoprops(gc, \"contrast\")[0, 0],\n",
    "        graycoprops(gc, \"homogeneity\")[0, 0]\n",
    "    ])\n",
    "\n",
    "\n",
    "def extract_sift_frame(gray_frame):\n",
    "    \"\"\"extract SIFT features from a single grayscale frame\"\"\"\n",
    "    # create sift detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # detect keypoints and compute descriptors\n",
    "    kp, des = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    # handle frames where sift finds nothing\n",
    "    if des is None:\n",
    "        return np.zeros(128)   # sift descriptor size is always 128\n",
    "\n",
    "    # average all descriptors to get a fixed-size feature vector\n",
    "    return des.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b980f8",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823ec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_vectorized(audio_path, fps, total_frames, n_mfcc=20):\n",
    "    # 1. Load the full audio track\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # 2. Calculate hop_length to sync with video FPS\n",
    "    # This aligns the audio analysis window exactly with the video frame rate\n",
    "    hop_length = int(sr / fps)\n",
    "    \n",
    "    # 3. Compute Features globally (Vectorized)\n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    \n",
    "    # Spectral Features\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)\n",
    "    bw = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)\n",
    "    con = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Spectral Flux (Onset Strength)\n",
    "    flux = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # 4. Helper to transpose and fix length mismatch\n",
    "    # Librosa often returns +/- 1 frame compared to OpenCV due to rounding.\n",
    "    def fix_and_transpose(features, target_len):\n",
    "        # Transpose so shape becomes (n_frames, n_features)\n",
    "        if features.ndim > 1:\n",
    "            features = features.T\n",
    "        else:\n",
    "            features = features.reshape(-1, 1)\n",
    "            \n",
    "        current_len = features.shape[0]\n",
    "        \n",
    "        # If audio is shorter than video, pad with zeros\n",
    "        if current_len < target_len:\n",
    "            pad_width = target_len - current_len\n",
    "            # Pad the time dimension (axis 0)\n",
    "            if features.ndim == 2:\n",
    "                features = np.pad(features, ((0, pad_width), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                features = np.pad(features, (0, pad_width), mode='constant')\n",
    "                \n",
    "        # If audio is longer, just trim it\n",
    "        return features[:target_len]\n",
    "\n",
    "    # 5. Return dict (Note: Spectral Contrast has 7 bands, so we stack it with others)\n",
    "    # We stack the scalar spectral features (centroid, bandwidth) for easier handling\n",
    "    spec_stacked = np.hstack([\n",
    "        fix_and_transpose(cent, total_frames),\n",
    "        fix_and_transpose(bw, total_frames),\n",
    "        fix_and_transpose(con, total_frames)\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"mfcc\":   fix_and_transpose(mfcc, total_frames),\n",
    "        \"spec\":   spec_stacked,\n",
    "        \"chroma\": fix_and_transpose(chroma, total_frames),\n",
    "        \"flux\":   fix_and_transpose(flux, total_frames)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70820eb3",
   "metadata": {},
   "source": [
    "### Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2b0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE = \"datasets_cache.npz\"\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(f\"Cache found at {CACHE_FILE}. Loading finished datasets...\")\n",
    "    with np.load(CACHE_FILE) as data:\n",
    "        ds_visual   = data['visual']\n",
    "        ds_audio    = data['audio']\n",
    "        ds_combined = data['combined']\n",
    "        y_labels    = data['y']\n",
    "        \n",
    "    print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a523a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video: 211\n",
      "processing video: 244\n",
      "processing video: 343\n",
      "Saving datasets to datasets_cache.npz...\n"
     ]
    }
   ],
   "source": [
    "# visual features\n",
    "hog_features   = []\n",
    "lbp_features   = []\n",
    "glcm_features  = []\n",
    "sift_features  = []\n",
    "\n",
    "# audio features\n",
    "mfcc_frames    = []\n",
    "spec_frames    = []\n",
    "chroma_frames  = []\n",
    "flux_frames    = []\n",
    "\n",
    "frame_meta       = []  # list of (video_id, frame_idx)\n",
    "labels_per_frame = []\n",
    "\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video_id = int(os.path.splitext(os.path.basename(video_path))[0])\n",
    "    print(\"processing video:\", video_id)\n",
    "\n",
    "    # get fps and total frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    # load per-frame audio features\n",
    "    audio_path = f\"./data/audio/{video_id}.wav\"\n",
    "    audio_feats = extract_audio_features_vectorized(\n",
    "        audio_path,\n",
    "        fps,\n",
    "        total_frames,\n",
    "        n_mfcc=20\n",
    "    )\n",
    "\n",
    "    # loop over frames and grab both visual+audio features\n",
    "    for frame_idx, frame_bgr, fps, total_frames in video_iter_frames(video_path):\n",
    "        # get label\n",
    "        label = get_label(video_id, frame_idx, gt_df)\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        frame_bgr = cv2.resize(frame_bgr, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # convert frame to grayscale\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        grayscale = color.rgb2gray(frame_rgb)\n",
    "        grayscale = (grayscale * 255).astype(np.uint8)\n",
    "\n",
    "        # meta + label\n",
    "        frame_meta.append((video_id, frame_idx))\n",
    "        labels_per_frame.append(label)\n",
    "\n",
    "        # visual features\n",
    "        hog_features.append(extract_hog_frame(grayscale))\n",
    "        lbp_features.append(extract_lbp_frame(grayscale))\n",
    "        glcm_features.append(extract_glcm_frame(grayscale))\n",
    "        sift_features.append(extract_sift_frame(grayscale))\n",
    "\n",
    "        # audio features: just index by frame_idx\n",
    "        mfcc_frames.append(audio_feats[\"mfcc\"][frame_idx])\n",
    "        spec_frames.append(audio_feats[\"spec\"][frame_idx])\n",
    "        chroma_frames.append(audio_feats[\"chroma\"][frame_idx])\n",
    "        flux_frames.append(audio_feats[\"flux\"][frame_idx])\n",
    "\n",
    "# convert to arrays\n",
    "X_hog_frames = np.vstack(hog_features)\n",
    "X_lbp_frames = np.vstack(lbp_features)\n",
    "X_glcm_frames = np.vstack(glcm_features)\n",
    "X_sift_frames = np.vstack(sift_features)\n",
    "\n",
    "X_mfcc_frames = np.vstack(mfcc_frames)\n",
    "X_spec_frames = np.vstack(spec_frames)\n",
    "X_chroma_frames = np.vstack(chroma_frames)\n",
    "X_flux_frames = np.vstack(flux_frames)\n",
    "\n",
    "y_labels = np.array(labels_per_frame)\n",
    "\n",
    "ds_visual = np.hstack((X_hog_frames, X_lbp_frames, X_sift_frames))\n",
    "ds_audio  = np.hstack((X_mfcc_frames, X_spec_frames, X_chroma_frames))\n",
    "ds_combined = np.hstack((ds_visual, ds_audio))\n",
    "\n",
    "# Save to cache\n",
    "print(f\"Saving datasets to {CACHE_FILE}...\")\n",
    "np.savez_compressed(\n",
    "    CACHE_FILE,\n",
    "    visual=ds_visual,\n",
    "    audio=ds_audio,\n",
    "    combined=ds_combined,\n",
    "    y=y_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010a2d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ visual feature matrices ================\n",
      "X_hog_frames   : (19877, 8100)\n",
      "X_lbp_frames   : (19877, 10)\n",
      "X_glcm_frames  : (19877, 2)\n",
      "X_sift_frames  : (19877, 128)\n",
      "\n",
      "================ audio feature matrices =================\n",
      "X_mfcc_frames  : (19877, 20)\n",
      "X_spec_frames  : (19877, 9)\n",
      "X_chroma_frames: (19877, 12)\n",
      "X_flux_frames  : (19877, 1)\n",
      "\n",
      "number of labels: 19877\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ visual feature matrices ================\")\n",
    "print(\"X_hog_frames   :\", X_hog_frames.shape)\n",
    "print(\"X_lbp_frames   :\", X_lbp_frames.shape)\n",
    "print(\"X_glcm_frames  :\", X_glcm_frames.shape)\n",
    "print(\"X_sift_frames  :\", X_sift_frames.shape)\n",
    "\n",
    "print(\"\\n================ audio feature matrices =================\")\n",
    "print(\"X_mfcc_frames  :\", X_mfcc_frames.shape)\n",
    "print(\"X_spec_frames  :\", X_spec_frames.shape)\n",
    "print(\"X_chroma_frames:\", X_chroma_frames.shape)\n",
    "print(\"X_flux_frames  :\", X_flux_frames.shape)\n",
    "\n",
    "print(\"\\nnumber of labels:\", len(y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad1505",
   "metadata": {},
   "source": [
    "### Comparing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e9544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_feature(X, y, name):\n",
    "    # split once, same split for all features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    # scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "\n",
    "    # svm classifier\n",
    "    clf = SVC(kernel=\"rbf\", C=10, gamma=\"scale\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99cdf066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- visual feature comparison ----\n",
      "HOG           f1 = 0.9997\n",
      "LBP           f1 = 0.9836\n",
      "GLCM          f1 = 0.8156\n",
      "SIFT          f1 = 0.9891\n",
      "\n",
      "---- audio feature comparison ----\n",
      "MFCC          f1 = 0.8296\n",
      "Spectral      f1 = 0.5399\n",
      "Chroma        f1 = 0.4268\n",
      "Flux          f1 = 0.2497\n",
      "\n",
      "===== ranking: visual features =====\n",
      "HOG           f1 = 0.9997\n",
      "SIFT          f1 = 0.9891\n",
      "LBP           f1 = 0.9836\n",
      "GLCM          f1 = 0.8156\n",
      "\n",
      "===== ranking: audio features =====\n",
      "MFCC          f1 = 0.8296\n",
      "Spectral      f1 = 0.5399\n",
      "Chroma        f1 = 0.4268\n",
      "Flux          f1 = 0.2497\n"
     ]
    }
   ],
   "source": [
    "visual_features = [\n",
    "    (X_hog_frames,   \"HOG\"),\n",
    "    (X_lbp_frames,   \"LBP\"),\n",
    "    (X_glcm_frames,  \"GLCM\"),\n",
    "    (X_sift_frames,  \"SIFT\"),\n",
    "]\n",
    "\n",
    "audio_features = [\n",
    "    (X_mfcc_frames,   \"MFCC\"),\n",
    "    (X_spec_frames,   \"Spectral\"),\n",
    "    (X_chroma_frames, \"Chroma\"),\n",
    "    (X_flux_frames,   \"Flux\"),\n",
    "]\n",
    "\n",
    "print(\"\\n---- visual feature comparison ----\")\n",
    "visual_scores = []\n",
    "for X, name in visual_features:\n",
    "    f1 = evaluate_feature(X, y_labels, name)\n",
    "    visual_scores.append((name, f1))\n",
    "\n",
    "print(\"\\n---- audio feature comparison ----\")\n",
    "audio_scores = []\n",
    "for X, name in audio_features:\n",
    "    f1 = evaluate_feature(X, y_labels, name)\n",
    "    audio_scores.append((name, f1))\n",
    "\n",
    "visual_ranked = sorted(visual_scores, key=lambda x: x[1], reverse=True)\n",
    "audio_ranked  = sorted(audio_scores,  key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n===== ranking: visual features =====\")\n",
    "for name, f1 in visual_ranked:\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")\n",
    "\n",
    "print(\"\\n===== ranking: audio features =====\")\n",
    "for name, f1 in audio_ranked:\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ea7a3",
   "metadata": {},
   "source": [
    "SIFT and HOG achieve near-perfect visual classification performance, while MFCC is the only audio feature that meaningfully contributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005119f1",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2451843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transform(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c70b8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_svm = SVC(kernel=\"rbf\", C=10, gamma=\"scale\", random_state=SEED)\n",
    "cls_rf = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_depth=10, random_state=SEED)\n",
    "cls_mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=200, early_stopping=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c010dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visual_train, ds_visual_test, y_visual_train, y_visual_test = split_transform(ds_visual, y_labels)\n",
    "ds_audio_train, ds_audio_test, y_audio_train, y_audio_test = split_transform(ds_audio, y_labels)\n",
    "ds_combined_train, ds_combined_test, y_combined_train, y_combined_test = split_transform(ds_combined, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7e58a",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56d5e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VISUAL] + [SVM] -> F1-Score: 0.9997\n"
     ]
    }
   ],
   "source": [
    "# SVM / VISUAL\n",
    "cls_svm.fit(ds_visual_train, y_visual_train)\n",
    "pred_svm = cls_svm.predict(ds_visual_test)\n",
    "f1_svm = f1_score(y_visual_test, pred_svm, average=\"macro\")\n",
    "print(f\"[VISUAL] + [SVM] -> F1-Score: {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7c1145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDIO] + [SVM] -> F1-Score: 0.8273\n"
     ]
    }
   ],
   "source": [
    "# SVM / AUDIO\n",
    "cls_svm.fit(ds_audio_train, y_audio_train)\n",
    "pred_svm = cls_svm.predict(ds_audio_test)\n",
    "f1_svm = f1_score(y_audio_test, pred_svm, average=\"macro\")\n",
    "print(f\"[AUDIO] + [SVM] -> F1-Score: {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78370bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMBINED] + [SVM] -> F1-Score: 0.9997\n"
     ]
    }
   ],
   "source": [
    "# SVM / COMBINED\n",
    "cls_svm.fit(ds_combined_train, y_combined_train)\n",
    "pred_svm = cls_svm.predict(ds_combined_test)\n",
    "f1_svm = f1_score(y_combined_test, pred_svm, average=\"macro\")\n",
    "print(f\"[COMBINED] + [SVM] -> F1-Score: {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3433b",
   "metadata": {},
   "source": [
    "#### RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854feb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VISUAL] + [RF] -> F1-Score: 0.9621\n"
     ]
    }
   ],
   "source": [
    "# RF / VISUAL\n",
    "cls_rf.fit(ds_visual_train, y_visual_train)\n",
    "pred_rf = cls_rf.predict(ds_visual_test)\n",
    "f1_rf = f1_score(y_visual_test, pred_rf, average=\"macro\")\n",
    "print(f\"[VISUAL] + [RF] -> F1-Score: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a8f1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDIO] + [RF] -> F1-Score: 0.6513\n"
     ]
    }
   ],
   "source": [
    "# RF / AUDIO\n",
    "cls_rf.fit(ds_audio_train, y_audio_train)\n",
    "pred_rf = cls_rf.predict(ds_audio_test)\n",
    "f1_rf = f1_score(y_audio_test, pred_rf, average=\"macro\")\n",
    "print(f\"[AUDIO] + [RF] -> F1-Score: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb9b2d",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5201bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f48e738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VISUAL] + [MLP] -> F1-Score: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# MLP / VISUAL\n",
    "cls_mlp.fit(ds_visual_train, label_encoder.fit_transform(y_visual_train))\n",
    "pred_mlp = cls_mlp.predict(ds_visual_test)\n",
    "f1_mlp = f1_score(label_encoder.fit_transform(y_visual_test), pred_mlp, average=\"macro\")\n",
    "print(f\"[VISUAL] + [MLP] -> F1-Score: {f1_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6152f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MLP / AUDIO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcls_mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_audio_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_audio_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pred_mlp \u001b[38;5;241m=\u001b[39m cls_mlp\u001b[38;5;241m.\u001b[39mpredict(ds_audio_test)\n\u001b[1;32m      4\u001b[0m f1_mlp \u001b[38;5;241m=\u001b[39m f1_score(y_audio_test, pred_mlp, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:849\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    828\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \n\u001b[1;32m    830\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:508\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, sample_weight, incremental)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Run the Stochastic optimization solver\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;129;01min\u001b[39;00m _STOCHASTIC_SOLVERS:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stochastic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincremental\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:748\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_stochastic\u001b[0;34m(self, X, y, sample_weight, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss = \u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_))\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# update no_improvement_count based on training loss or\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# validation score according to early_stopping\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_no_improvement_count\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_val\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# for learning rate that needs to be updated at iteration end\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39miteration_ends(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_)\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:798\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._update_no_improvement_count\u001b[0;34m(self, early_stopping, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_no_improvement_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, early_stopping, X, y, sample_weight):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stopping:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# compute validation score (can be NaN), use that for stopping\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_scores_\u001b[38;5;241m.\u001b[39mappend(val_score)\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1289\u001b[0m, in \u001b[0;36mMLPClassifier._score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_with_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_score\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/University/WS25/Sim12/Sim2/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:864\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._score_with_function\u001b[0;34m(self, X, y, sample_weight, score_function)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Input validation would remove feature names, so we disable it\u001b[39;00m\n\u001b[1;32m    862\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(y_pred)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score_function(y, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# MLP / AUDIO\n",
    "cls_mlp.fit(ds_audio_train, label_encoder.fit_transform(y_audio_train))\n",
    "pred_mlp = cls_mlp.predict(ds_audio_test)\n",
    "f1_mlp = f1_score(label_encoder.fit_transform(y_audio_test), pred_mlp, average=\"macro\")\n",
    "print(f\"[AUDIO] + [MLP] -> F1-Score: {f1_mlp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
