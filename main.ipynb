{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57f1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, glob, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "from skimage import color\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# config\n",
    "SEED = 12332287\n",
    "np.random.seed(SEED)\n",
    "\n",
    "CLASSES = [\"MissPiggy\", \"OtherPigs\", \"SwedishChef\", \"Rowlf\"]\n",
    "\n",
    "# helpers\n",
    "def list_files(directory):\n",
    "    \"\"\"list files in a folder\"\"\"\n",
    "    return sorted(glob.glob(directory + \"/*\"))\n",
    "\n",
    "GT_REQUIRED = [\n",
    "    \"Video\", \"Frame_number\", \"Timestamp\",\n",
    "    \"Kermit\", \"Pigs\", \"Miss Piggy\", \"Cook\",\n",
    "    \"StatlerWaldorf\", \"Rowlf the Dog\", \"Fozzie Bear\"\n",
    "]\n",
    "\n",
    "GT_TO_INTERNAL = {\n",
    "    \"Miss Piggy\": \"MissPiggy\",\n",
    "    \"Pigs\": \"OtherPigs\",\n",
    "    \"Cook\": \"SwedishChef\",\n",
    "    \"Rowlf the Dog\": \"Rowlf\",\n",
    "}\n",
    "\n",
    "def read_ground_truth(gt_dir):\n",
    "    \"\"\"reads all ground truth xlsx files and returns df indexed by (Video, Frame_number)\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(gt_dir, \"*.xlsx\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"no ground truth xlsx found in gt_dir\")\n",
    "\n",
    "    dfs = []\n",
    "    for path in files:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "        # verify headers\n",
    "        missing = [c for c in GT_REQUIRED if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{os.path.basename(path)} missing columns: {missing}\")\n",
    "\n",
    "        # enforce numeric dtypes\n",
    "        df[\"Video\"] = pd.to_numeric(df[\"Video\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"Frame_number\"] = pd.to_numeric(df[\"Frame_number\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # map exact GT columns to internal class names\n",
    "        for gt_col, internal in GT_TO_INTERNAL.items():\n",
    "            df[internal] = pd.to_numeric(df[gt_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        dfs.append(df[[\"Video\", \"Frame_number\"] + list(GT_TO_INTERNAL.values())])\n",
    "\n",
    "    gt = pd.concat(dfs, ignore_index=True)\n",
    "    gt = gt.dropna(subset=[\"Video\", \"Frame_number\"])\n",
    "    gt[\"Video\"] = gt[\"Video\"].astype(int)\n",
    "    gt[\"Frame_number\"] = gt[\"Frame_number\"].astype(int)\n",
    "\n",
    "    gt = gt.set_index([\"Video\", \"Frame_number\"]).sort_index()\n",
    "\n",
    "    for c in CLASSES:\n",
    "        if c not in gt.columns:\n",
    "            gt[c] = 0\n",
    "        gt[c] = gt[c].astype(int)\n",
    "\n",
    "    return gt\n",
    "\n",
    "def get_label(video_id, frame_idx, gt_df):\n",
    "    \"\"\"\n",
    "    return one of the internal labels in CLASSES or None\n",
    "    based on the ground truth table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = gt_df.loc[(video_id, frame_idx)]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "    # row contains columns: MissPiggy, OtherPigs, SwedishChef, Rowlf\n",
    "    # only accept frames where exactly one of these is 1\n",
    "    vals = row[CLASSES].values.astype(int)\n",
    "    if vals.sum() != 1:\n",
    "        return None\n",
    "\n",
    "    label_idx = vals.argmax()\n",
    "    return CLASSES[label_idx]\n",
    "\n",
    "def video_iter_frames(path):\n",
    "    \"\"\"go through video frame by frame and yield (idx, frame_bgr, fps, nframes)\"\"\"\n",
    "    capture = cv2.VideoCapture(path) # open video\n",
    "    if not capture.isOpened():\n",
    "        raise RuntimeError(f\"cannot open video: {path}\")\n",
    "\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS) # frames per second\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_index = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = capture.read() # read next frame\n",
    "            if not ok:\n",
    "                break\n",
    "            yield frame_index, frame, fps, total_frames\n",
    "            frame_index += 1\n",
    "    finally:\n",
    "        capture.release()\n",
    "\n",
    "def video_to_audio(video_path):\n",
    "    \"\"\"extract audio from video and save as wav file\"\"\"\n",
    "    fname = os.path.basename(video_path).replace(\".avi\", \".wav\")\n",
    "    audio_out = os.path.join(\"data/audio\", fname)  # save in data/audio\n",
    "\n",
    "    # load video file and extract audio\n",
    "    video = mp.VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_out, codec=\"pcm_s16le\")\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d2464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = read_ground_truth(\"./data/ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def21c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in data/audio/211.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/244.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/343.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# separating audio from video\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "for v in video_paths:\n",
    "    video_to_audio(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e84a0",
   "metadata": {},
   "source": [
    "### Visual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4abd2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_frame(gray_frame):\n",
    "    \"\"\"extract HOG features from a single grayscale frame\"\"\"\n",
    "    h = hog(gray_frame, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))\n",
    "    return h\n",
    "\n",
    "\n",
    "def extract_lbp_frame(gray_frame, P=8, R=1):\n",
    "    \"\"\"extract LBP features from a single grayscale frame\"\"\"\n",
    "    lbp = local_binary_pattern(gray_frame, P, R, method=\"uniform\") # compute lbp codes for each pixel\n",
    "\n",
    "    # lbp produces values in [0, P+1], build histogram as the feature\n",
    "    bins = P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def extract_glcm_frame(gray_frame):\n",
    "    \"\"\"extract GLCM features from a single grayscale frame\"\"\"\n",
    "    # compute grey-level co-occurrence matrix for distance=1, angle=0\n",
    "    gc = graycomatrix(gray_frame, [1], [0], symmetric=True, normed=True) \n",
    "\n",
    "    # extract two common texture measures: contrast + homogeneity\n",
    "    return np.array([\n",
    "        graycoprops(gc, \"contrast\")[0, 0],\n",
    "        graycoprops(gc, \"homogeneity\")[0, 0]\n",
    "    ])\n",
    "\n",
    "\n",
    "def extract_sift_frame(gray_frame):\n",
    "    \"\"\"extract SIFT features from a single grayscale frame\"\"\"\n",
    "    # create sift detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # detect keypoints and compute descriptors\n",
    "    kp, des = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    # handle frames where sift finds nothing\n",
    "    if des is None:\n",
    "        return np.zeros(128)   # sift descriptor size is always 128\n",
    "\n",
    "    # average all descriptors to get a fixed-size feature vector\n",
    "    return des.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b980f8",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823ec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_vectorized(audio_path, fps, total_frames, n_mfcc=20):\n",
    "    # 1. Load the full audio track\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # 2. Calculate hop_length to sync with video FPS\n",
    "    # This aligns the audio analysis window exactly with the video frame rate\n",
    "    hop_length = int(sr / fps)\n",
    "    \n",
    "    # 3. Compute Features globally (Vectorized)\n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    \n",
    "    # Spectral Features\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)\n",
    "    bw = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)\n",
    "    con = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Spectral Flux (Onset Strength)\n",
    "    flux = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # 4. Helper to transpose and fix length mismatch\n",
    "    # Librosa often returns +/- 1 frame compared to OpenCV due to rounding.\n",
    "    def fix_and_transpose(features, target_len):\n",
    "        # Transpose so shape becomes (n_frames, n_features)\n",
    "        if features.ndim > 1:\n",
    "            features = features.T\n",
    "        else:\n",
    "            features = features.reshape(-1, 1)\n",
    "            \n",
    "        current_len = features.shape[0]\n",
    "        \n",
    "        # If audio is shorter than video, pad with zeros\n",
    "        if current_len < target_len:\n",
    "            pad_width = target_len - current_len\n",
    "            # Pad the time dimension (axis 0)\n",
    "            if features.ndim == 2:\n",
    "                features = np.pad(features, ((0, pad_width), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                features = np.pad(features, (0, pad_width), mode='constant')\n",
    "                \n",
    "        # If audio is longer, just trim it\n",
    "        return features[:target_len]\n",
    "\n",
    "    # 5. Return dict (Note: Spectral Contrast has 7 bands, so we stack it with others)\n",
    "    # We stack the scalar spectral features (centroid, bandwidth) for easier handling\n",
    "    spec_stacked = np.hstack([\n",
    "        fix_and_transpose(cent, total_frames),\n",
    "        fix_and_transpose(bw, total_frames),\n",
    "        fix_and_transpose(con, total_frames)\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"mfcc\":   fix_and_transpose(mfcc, total_frames),\n",
    "        \"spec\":   spec_stacked,\n",
    "        \"chroma\": fix_and_transpose(chroma, total_frames),\n",
    "        \"flux\":   fix_and_transpose(flux, total_frames)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70820eb3",
   "metadata": {},
   "source": [
    "### Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a523a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video: 211\n",
      "processing video: 244\n",
      "processing video: 343\n"
     ]
    }
   ],
   "source": [
    "# visual features\n",
    "hog_features   = []\n",
    "lbp_features   = []\n",
    "glcm_features  = []\n",
    "sift_features  = []\n",
    "\n",
    "# audio features\n",
    "mfcc_frames    = []\n",
    "spec_frames    = []\n",
    "chroma_frames  = []\n",
    "flux_frames    = []\n",
    "\n",
    "frame_meta       = []  # list of (video_id, frame_idx)\n",
    "labels_per_frame = []\n",
    "\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video_id = int(os.path.splitext(os.path.basename(video_path))[0])\n",
    "    print(\"processing video:\", video_id)\n",
    "\n",
    "    # get fps and total frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    # load per-frame audio features\n",
    "    audio_path = f\"./data/audio/{video_id}.wav\"\n",
    "    audio_feats = extract_audio_features_vectorized(\n",
    "        audio_path,\n",
    "        fps,\n",
    "        total_frames,\n",
    "        n_mfcc=20\n",
    "    )\n",
    "\n",
    "    # loop over frames and grab both visual+audio features\n",
    "    for frame_idx, frame_bgr, fps, total_frames in video_iter_frames(video_path):\n",
    "        # get label\n",
    "        label = get_label(video_id, frame_idx, gt_df)\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        frame_bgr = cv2.resize(frame_bgr, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # convert frame to grayscale\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        grayscale = color.rgb2gray(frame_rgb)\n",
    "        grayscale = (grayscale * 255).astype(np.uint8)\n",
    "\n",
    "        # meta + label\n",
    "        frame_meta.append((video_id, frame_idx))\n",
    "        labels_per_frame.append(label)\n",
    "\n",
    "        # visual features\n",
    "        hog_features.append(extract_hog_frame(grayscale))\n",
    "        lbp_features.append(extract_lbp_frame(grayscale))\n",
    "        glcm_features.append(extract_glcm_frame(grayscale))\n",
    "        sift_features.append(extract_sift_frame(grayscale))\n",
    "\n",
    "        # audio features: just index by frame_idx\n",
    "        mfcc_frames.append(audio_feats[\"mfcc\"][frame_idx])\n",
    "        spec_frames.append(audio_feats[\"spec\"][frame_idx])\n",
    "        chroma_frames.append(audio_feats[\"chroma\"][frame_idx])\n",
    "        flux_frames.append(audio_feats[\"flux\"][frame_idx])\n",
    "\n",
    "# convert to arrays\n",
    "X_hog_frames = np.vstack(hog_features)\n",
    "X_lbp_frames = np.vstack(lbp_features)\n",
    "X_glcm_frames = np.vstack(glcm_features)\n",
    "X_sift_frames = np.vstack(sift_features)\n",
    "\n",
    "X_mfcc_frames = np.vstack(mfcc_frames)\n",
    "X_spec_frames = np.vstack(spec_frames)\n",
    "X_chroma_frames = np.vstack(chroma_frames)\n",
    "X_flux_frames = np.vstack(flux_frames)\n",
    "\n",
    "y_labels = np.array(labels_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010a2d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ visual feature matrices ================\n",
      "X_hog_frames   : (19877, 8100)\n",
      "X_lbp_frames   : (19877, 10)\n",
      "X_glcm_frames  : (19877, 2)\n",
      "X_sift_frames  : (19877, 128)\n",
      "\n",
      "================ audio feature matrices =================\n",
      "X_mfcc_frames  : (19877, 20)\n",
      "X_spec_frames  : (19877, 9)\n",
      "X_chroma_frames: (19877, 12)\n",
      "X_flux_frames  : (19877, 1)\n",
      "\n",
      "number of labels: 19877\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ visual feature matrices ================\")\n",
    "print(\"X_hog_frames   :\", X_hog_frames.shape)\n",
    "print(\"X_lbp_frames   :\", X_lbp_frames.shape)\n",
    "print(\"X_glcm_frames  :\", X_glcm_frames.shape)\n",
    "print(\"X_sift_frames  :\", X_sift_frames.shape)\n",
    "\n",
    "print(\"\\n================ audio feature matrices =================\")\n",
    "print(\"X_mfcc_frames  :\", X_mfcc_frames.shape)\n",
    "print(\"X_spec_frames  :\", X_spec_frames.shape)\n",
    "print(\"X_chroma_frames:\", X_chroma_frames.shape)\n",
    "print(\"X_flux_frames  :\", X_flux_frames.shape)\n",
    "\n",
    "print(\"\\nnumber of labels:\", len(y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad1505",
   "metadata": {},
   "source": [
    "### Comparing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e9544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_feature(X, y, name):\n",
    "    # split once, same split for all features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    # scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "\n",
    "    # svm classifier\n",
    "    clf = SVC(kernel=\"rbf\", C=10, gamma=\"scale\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cdf066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- visual feature comparison ----\n",
      "HOG           f1 = 0.9997\n",
      "LBP           f1 = 0.9836\n",
      "GLCM          f1 = 0.8156\n",
      "SIFT          f1 = 0.9891\n",
      "\n",
      "---- audio feature comparison ----\n",
      "MFCC          f1 = 0.8296\n",
      "Spectral      f1 = 0.5399\n",
      "Chroma        f1 = 0.4268\n",
      "Flux          f1 = 0.2497\n",
      "\n",
      "===== ranking: visual features =====\n",
      "HOG           f1 = 0.9997\n",
      "SIFT          f1 = 0.9891\n",
      "LBP           f1 = 0.9836\n",
      "GLCM          f1 = 0.8156\n",
      "\n",
      "===== ranking: audio features =====\n",
      "MFCC          f1 = 0.8296\n",
      "Spectral      f1 = 0.5399\n",
      "Chroma        f1 = 0.4268\n",
      "Flux          f1 = 0.2497\n"
     ]
    }
   ],
   "source": [
    "visual_features = [\n",
    "    (X_hog_frames,   \"HOG\"),\n",
    "    (X_lbp_frames,   \"LBP\"),\n",
    "    (X_glcm_frames,  \"GLCM\"),\n",
    "    (X_sift_frames,  \"SIFT\"),\n",
    "]\n",
    "\n",
    "audio_features = [\n",
    "    (X_mfcc_frames,   \"MFCC\"),\n",
    "    (X_spec_frames,   \"Spectral\"),\n",
    "    (X_chroma_frames, \"Chroma\"),\n",
    "    (X_flux_frames,   \"Flux\"),\n",
    "]\n",
    "\n",
    "print(\"\\n---- visual feature comparison ----\")\n",
    "visual_scores = []\n",
    "for X, name in visual_features:\n",
    "    f1 = evaluate_feature(X, y_labels, name)\n",
    "    visual_scores.append((name, f1))\n",
    "\n",
    "print(\"\\n---- audio feature comparison ----\")\n",
    "audio_scores = []\n",
    "for X, name in audio_features:\n",
    "    f1 = evaluate_feature(X, y_labels, name)\n",
    "    audio_scores.append((name, f1))\n",
    "\n",
    "visual_ranked = sorted(visual_scores, key=lambda x: x[1], reverse=True)\n",
    "audio_ranked  = sorted(audio_scores,  key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n===== ranking: visual features =====\")\n",
    "for name, f1 in visual_ranked:\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")\n",
    "\n",
    "print(\"\\n===== ranking: audio features =====\")\n",
    "for name, f1 in audio_ranked:\n",
    "    print(f\"{name:12s}  f1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ea7a3",
   "metadata": {},
   "source": [
    "SIFT and HOG achieve near-perfect visual classification performance, while MFCC is the only audio feature that meaningfully contributes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
