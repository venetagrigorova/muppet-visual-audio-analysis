{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, glob, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "from skimage import color\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# config\n",
    "SEED = 12332287\n",
    "np.random.seed(SEED)\n",
    "\n",
    "CLASSES = [\"MissPiggy\", \"OtherPigs\", \"SwedishChef\", \"Rowlf\"]\n",
    "\n",
    "# helpers\n",
    "def list_files(directory):\n",
    "    \"\"\"list files in a folder\"\"\"\n",
    "    return sorted(glob.glob(directory + \"/*\"))\n",
    "\n",
    "GT_REQUIRED = [\n",
    "    \"Video\", \"Frame_number\", \"Timestamp\",\n",
    "    \"Kermit\", \"Pigs\", \"Miss Piggy\", \"Cook\",\n",
    "    \"StatlerWaldorf\", \"Rowlf the Dog\", \"Fozzie Bear\"\n",
    "]\n",
    "\n",
    "GT_TO_INTERNAL = {\n",
    "    \"Miss Piggy\": \"MissPiggy\",\n",
    "    \"Pigs\": \"OtherPigs\",\n",
    "    \"Cook\": \"SwedishChef\",\n",
    "    \"Rowlf the Dog\": \"Rowlf\",\n",
    "}\n",
    "\n",
    "def read_ground_truth(gt_dir):\n",
    "    \"\"\"reads all ground truth xlsx files and returns df indexed by (Video, Frame_number)\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(gt_dir, \"*.xlsx\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"no ground truth xlsx found in gt_dir\")\n",
    "\n",
    "    dfs = []\n",
    "    for path in files:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "        # verify headers\n",
    "        missing = [c for c in GT_REQUIRED if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{os.path.basename(path)} missing columns: {missing}\")\n",
    "\n",
    "        # enforce numeric dtypes\n",
    "        df[\"Video\"] = pd.to_numeric(df[\"Video\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"Frame_number\"] = pd.to_numeric(df[\"Frame_number\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # map exact GT columns to internal class names\n",
    "        for gt_col, internal in GT_TO_INTERNAL.items():\n",
    "            df[internal] = pd.to_numeric(df[gt_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        dfs.append(df[[\"Video\", \"Frame_number\"] + list(GT_TO_INTERNAL.values())])\n",
    "\n",
    "    gt = pd.concat(dfs, ignore_index=True)\n",
    "    gt = gt.dropna(subset=[\"Video\", \"Frame_number\"])\n",
    "    gt[\"Video\"] = gt[\"Video\"].astype(int)\n",
    "    gt[\"Frame_number\"] = gt[\"Frame_number\"].astype(int)\n",
    "\n",
    "    gt = gt.set_index([\"Video\", \"Frame_number\"]).sort_index()\n",
    "\n",
    "    for c in CLASSES:\n",
    "        if c not in gt.columns:\n",
    "            gt[c] = 0\n",
    "        gt[c] = gt[c].astype(int)\n",
    "\n",
    "    return gt\n",
    "\n",
    "def get_label(video_id, frame_idx, gt_df):\n",
    "    \"\"\"\n",
    "    return one of the internal labels in CLASSES or None\n",
    "    based on the ground truth table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = gt_df.loc[(video_id, frame_idx)]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "    # row contains columns: MissPiggy, OtherPigs, SwedishChef, Rowlf\n",
    "    # only accept frames where exactly one of these is 1\n",
    "    vals = row[CLASSES].values.astype(int)\n",
    "    if vals.sum() != 1:\n",
    "        return None\n",
    "\n",
    "    label_idx = vals.argmax()\n",
    "    return CLASSES[label_idx]\n",
    "\n",
    "def video_iter_frames(path):\n",
    "    \"\"\"go through video frame by frame and yield (idx, frame_bgr, fps, nframes)\"\"\"\n",
    "    capture = cv2.VideoCapture(path) # open video\n",
    "    if not capture.isOpened():\n",
    "        raise RuntimeError(f\"cannot open video: {path}\")\n",
    "\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS) # frames per second\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_index = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = capture.read() # read next frame\n",
    "            if not ok:\n",
    "                break\n",
    "            yield frame_index, frame, fps, total_frames\n",
    "            frame_index += 1\n",
    "    finally:\n",
    "        capture.release()\n",
    "\n",
    "def video_to_audio(video_path):\n",
    "    \"\"\"extract audio from video and save as wav file\"\"\"\n",
    "    fname = os.path.basename(video_path).replace(\".avi\", \".wav\")\n",
    "    audio_out = os.path.join(\"data/audio\", fname)  # save in data/audio\n",
    "\n",
    "    # load video file and extract audio\n",
    "    video = mp.VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_out, codec=\"pcm_s16le\")\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d2464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = read_ground_truth(\"./data/ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def21c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in data/audio/211.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/244.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in data/audio/343.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# separating audio from video\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "for v in video_paths:\n",
    "    video_to_audio(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e84a0",
   "metadata": {},
   "source": [
    "### Visual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_frame(gray_frame, n=8100):\n",
    "    \"\"\"extract HOG features from a single grayscale frame\"\"\"\n",
    "    # hog returns a long vector, we keep only the first n dims for consistency\n",
    "    h = hog(gray_frame, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))\n",
    "    return h[:n]\n",
    "\n",
    "\n",
    "def extract_lbp_frame(gray_frame, P=8, R=1):\n",
    "    \"\"\"extract LBP features from a single grayscale frame\"\"\"\n",
    "    lbp = local_binary_pattern(gray_frame, P, R, method=\"uniform\") # compute lbp codes for each pixel\n",
    "\n",
    "    # lbp produces values in [0, P+1], build histogram as the feature\n",
    "    bins = P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def extract_glcm_frame(gray_frame):\n",
    "    \"\"\"extract GLCM features from a single grayscale frame\"\"\"\n",
    "    # compute grey-level co-occurrence matrix for distance=1, angle=0\n",
    "    gc = graycomatrix(gray_frame, [1], [0], symmetric=True, normed=True) \n",
    "\n",
    "    # extract two common texture measures: contrast + homogeneity\n",
    "    return np.array([\n",
    "        graycoprops(gc, \"contrast\")[0, 0],\n",
    "        graycoprops(gc, \"homogeneity\")[0, 0]\n",
    "    ])\n",
    "\n",
    "\n",
    "def extract_sift_frame(gray_frame):\n",
    "    \"\"\"extract SIFT features from a single grayscale frame\"\"\"\n",
    "    # create sift detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # detect keypoints and compute descriptors\n",
    "    kp, des = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    # handle frames where sift finds nothing\n",
    "    if des is None:\n",
    "        return np.zeros(128)   # sift descriptor size is always 128\n",
    "\n",
    "    # average all descriptors to get a fixed-size feature vector\n",
    "    return des.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b980f8",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_per_frame(audio_path, fps, total_frames, n_mfcc=20):\n",
    "    \"\"\"extract audio features per video frame from an audio file\"\"\"\n",
    "    audio_waveform, sampling_rate = librosa.load(audio_path, sr=None) # load full audio\n",
    "\n",
    "    frame_len = int(sampling_rate / fps)  # samples per video frame\n",
    "\n",
    "    mfcc_list   = []\n",
    "    spec_list   = []\n",
    "    chroma_list = []\n",
    "    flux_list   = []\n",
    "\n",
    "    for frame in range(total_frames):\n",
    "        start = frame * frame_len\n",
    "        end   = start + frame_len\n",
    "        audio_frame = audio_waveform[start:end]\n",
    "\n",
    "        # empty slice fallback\n",
    "        if len(audio_frame) < 5:\n",
    "            mfcc_list.append(np.zeros(n_mfcc*2))\n",
    "            spec_list.append(np.zeros(8))\n",
    "            chroma_list.append(np.zeros(24))\n",
    "            flux_list.append(np.zeros(2))\n",
    "            continue\n",
    "\n",
    "        # mfcc\n",
    "        mf = librosa.feature.mfcc(y=audio_frame, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "        mf = np.concatenate([mf.mean(1), mf.std(1)])\n",
    "        mfcc_list.append(mf)\n",
    "\n",
    "        # spectral\n",
    "        cent = librosa.feature.spectral_centroid(y=audio_frame, sr=sampling_rate)[0]\n",
    "        bw = librosa.feature.spectral_bandwidth(y=audio_frame, sr=sampling_rate)[0]\n",
    "        con = librosa.feature.spectral_contrast(y=audio_frame, sr=sampling_rate)\n",
    "        zc = librosa.feature.zero_crossing_rate(audio_frame)[0]\n",
    "        spec_vec = np.array([\n",
    "            cent.mean(), cent.std(),\n",
    "            bw.mean(), bw.std(),\n",
    "            con.mean(), con.std(),\n",
    "            zc.mean(), zc.std(),\n",
    "        ])\n",
    "        spec_list.append(spec_vec)\n",
    "\n",
    "        # chroma\n",
    "        ch = librosa.feature.chroma_stft(y=audio_frame, sr=sampling_rate)\n",
    "        chroma_list.append(np.concatenate([ch.mean(1), ch.std(1)]))\n",
    "\n",
    "        # spectral flux\n",
    "        fl = librosa.onset.onset_strength(y=audio_frame, sr=sampling_rate)\n",
    "        flux_list.append(np.array([fl.mean(), fl.std()]))\n",
    "\n",
    "    return {\n",
    "        \"mfcc\":   np.vstack(mfcc_list),\n",
    "        \"spec\":   np.vstack(spec_list),\n",
    "        \"chroma\": np.vstack(chroma_list),\n",
    "        \"flux\":   np.vstack(flux_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70820eb3",
   "metadata": {},
   "source": [
    "### Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual features\n",
    "hog_features   = []\n",
    "lbp_features   = []\n",
    "glcm_features  = []\n",
    "sift_features  = []\n",
    "\n",
    "# audio features\n",
    "mfcc_frames    = []\n",
    "spec_frames    = []\n",
    "chroma_frames  = []\n",
    "flux_frames    = []\n",
    "\n",
    "frame_meta       = []  # list of (video_id, frame_idx)\n",
    "labels_per_frame = []\n",
    "\n",
    "video_paths = list_files(\"./data/episodes\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video_id = int(os.path.splitext(os.path.basename(video_path))[0])\n",
    "    print(\"processing video:\", video_id)\n",
    "\n",
    "    # get fps and total frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    # load per-frame audio features\n",
    "    audio_path = f\"./data/audio/{video_id}.wav\"\n",
    "    audio_feats = extract_audio_features_per_frame(\n",
    "        audio_path,\n",
    "        fps,\n",
    "        total_frames,\n",
    "        n_mfcc=20\n",
    "    )\n",
    "\n",
    "    # loop over frames and grab both visual+audio features\n",
    "    for frame_idx, frame_bgr, fps, total_frames in video_iter_frames(video_path):\n",
    "        # get label\n",
    "        label = get_label(video_id, frame_idx, gt_df)\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        # convert frame to grayscale\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        grayscale = color.rgb2gray(frame_rgb)\n",
    "        grayscale = (grayscale * 255).astype(np.uint8)\n",
    "\n",
    "        # meta + label\n",
    "        frame_meta.append((video_id, frame_idx))\n",
    "        labels_per_frame.append(label)\n",
    "\n",
    "        # visual features\n",
    "        hog_features.append(extract_hog_frame(grayscale))\n",
    "        lbp_features.append(extract_lbp_frame(grayscale))\n",
    "        glcm_features.append(extract_glcm_frame(grayscale))\n",
    "        sift_features.append(extract_sift_frame(grayscale))\n",
    "\n",
    "        # audio features: just index by frame_idx\n",
    "        mfcc_frames.append(audio_feats[\"mfcc\"][frame_idx])\n",
    "        spec_frames.append(audio_feats[\"spec\"][frame_idx])\n",
    "        chroma_frames.append(audio_feats[\"chroma\"][frame_idx])\n",
    "        flux_frames.append(audio_feats[\"flux\"][frame_idx])\n",
    "\n",
    "# convert to arrays\n",
    "X_hog_frames = np.vstack(hog_features)\n",
    "X_lbp_frames = np.vstack(lbp_features)\n",
    "X_glcm_frames = np.vstack(glcm_features)\n",
    "X_sift_frames = np.vstack(sift_features)\n",
    "\n",
    "X_mfcc_frames = np.vstack(mfcc_frames)\n",
    "X_spec_frames = np.vstack(spec_frames)\n",
    "X_chroma_frames = np.vstack(chroma_frames)\n",
    "X_flux_frames = np.vstack(flux_frames)\n",
    "\n",
    "y_labels = np.array(labels_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "010a2d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ visual feature matrices ================\n",
      "X_hog_frames   : (19877, 8100)\n",
      "X_lbp_frames   : (19877, 10)\n",
      "X_glcm_frames  : (19877, 2)\n",
      "X_sift_frames  : (19877, 128)\n",
      "\n",
      "================ audio feature matrices =================\n",
      "X_mfcc_frames  : (19877, 40)\n",
      "X_spec_frames  : (19877, 8)\n",
      "X_chroma_frames: (19877, 24)\n",
      "X_flux_frames  : (19877, 2)\n",
      "\n",
      "number of labels: 19877\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ visual feature matrices ================\")\n",
    "print(\"X_hog_frames   :\", X_hog_frames.shape)\n",
    "print(\"X_lbp_frames   :\", X_lbp_frames.shape)\n",
    "print(\"X_glcm_frames  :\", X_glcm_frames.shape)\n",
    "print(\"X_sift_frames  :\", X_sift_frames.shape)\n",
    "\n",
    "print(\"\\n================ audio feature matrices =================\")\n",
    "print(\"X_mfcc_frames  :\", X_mfcc_frames.shape)\n",
    "print(\"X_spec_frames  :\", X_spec_frames.shape)\n",
    "print(\"X_chroma_frames:\", X_chroma_frames.shape)\n",
    "print(\"X_flux_frames  :\", X_flux_frames.shape)\n",
    "\n",
    "print(\"\\nnumber of labels:\", len(y_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
